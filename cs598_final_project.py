# -*- coding: utf-8 -*-
"""cs598-final-project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ObtzXwfrMXl3i3P_j394KyxC09bAjlCV
"""

from google.colab import drive
drive.mount('/content/gdrive')

!ls gdrive/MyDrive

!unzip gdrive/MyDrive/p11.zip

# prompt: I have a folder in my colab where there are text files within each subfolder of the folder. I want to get the contents of those text files into an Excel sheet along with the text file name in two columns

import os
import pandas as pd

def process_text_files(root_folder):
    data = []
    for dirpath, dirnames, filenames in os.walk(root_folder):
        for filename in filenames:
            if filename.endswith(".txt"):
                filepath = os.path.join(dirpath, filename)
                try:
                    with open(filepath, 'r', encoding='utf-8') as file:
                        content = file.read()
                        data.append([filename, content])
                except Exception as e:
                    print(f"Error processing {filepath}: {e}")
    df = pd.DataFrame(data, columns=['Filename', 'Content'])
    return df

# Example usage
root_folder = "p11" # Replace with the actual path to your root folder
df = process_text_files(root_folder)

# Save to excel
excel_filepath = "output.xlsx"
df.to_excel(excel_filepath, index=False)

print(f"Data saved to {excel_filepath}")

# prompt: I have three csvs in a folder called document_level_data. I want you to for each row, in the event that all three CSVs have the same label, denoted by the 'Classification' column for the same Filename, to add the Filename to a list and return that list

import pandas as pd
import os

def find_consistent_classifications(folder_path):
    """
    Finds filenames where the classification is consistent across three CSV files.

    Args:
        folder_path: The path to the folder containing the CSV files.

    Returns:
        A list of filenames with consistent classifications.
    """

    consistent_filenames = []
    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]

    if len(csv_files) != 3:
        print("Error: The folder must contain exactly three CSV files.")
        return consistent_filenames

    try:
        dfs = [pd.read_csv(os.path.join(folder_path, csv_file)) for csv_file in csv_files]
    except FileNotFoundError:
        print(f"Error: One or more CSV files not found in {folder_path}")
        return consistent_filenames
    except pd.errors.EmptyDataError:
        print(f"Error: One or more CSV files are empty in {folder_path}")
        return consistent_filenames
    except pd.errors.ParserError:
        print(f"Error: Could not parse one or more CSV files in {folder_path}")
        return consistent_filenames


    # Merge dataframes on 'Filename'
    merged_df = dfs[0].merge(dfs[1], on='Filename', suffixes=('_1', '_2'))
    merged_df = merged_df.merge(dfs[2], on='Filename')

    # Find rows where all classifications are the same
    for index, row in merged_df.iterrows():
        if row['Classification_1'] == row['Classification_2'] == row['Classification']:
            consistent_filenames.append(row['Filename'])

    return consistent_filenames

# Example usage:
folder_path = "document_level_data"  # Replace with your folder path
consistent_files = find_consistent_classifications(folder_path)
print(f"Filenames with consistent classifications: {consistent_files}")

# prompt: I want to filter a CSV and return another CSV where the 'Filename' column of each row is in the list passed into the function

import pandas as pd

def filter_csv_by_filenames(input_csv_path, filenames_to_keep, output_csv_path):
    """
    Filters a CSV file to include only rows where the 'Filename' column matches
    filenames in a provided list.

    Args:
        input_csv_path: Path to the input CSV file.
        filenames_to_keep: A list of filenames to include in the output.
        output_csv_path: Path to save the filtered CSV file.
    """
    try:
        df = pd.read_csv(input_csv_path)
    except FileNotFoundError:
        print(f"Error: Input CSV file not found at {input_csv_path}")
        return
    except pd.errors.ParserError:
        print(f"Error: Could not parse the input CSV file at {input_csv_path}")
        return


    # Filter the DataFrame
    filtered_df = df[df['Filename'].isin(filenames_to_keep)]

    # Save the filtered DataFrame to a new CSV file
    filtered_df.to_csv(output_csv_path, index=False)

    print(f"Filtered CSV saved to {output_csv_path}")

# Example usage:
input_csv_file = "document_level_data/p10_run1_doc.csv"  # Replace with your input CSV file path
# filenames = ["file1.txt", "file3.txt", "file5.txt"]  # Replace with your list of filenames
output_csv_file = "filtered_output.csv"  # Replace with desired output path

filter_csv_by_filenames(input_csv_file, consistent_files, output_csv_file)

# Load model directly
from transformers import AutoModel
model = AutoModel.from_pretrained("allenai/biomed_roberta_base")

pip install datasets

import pandas as pd
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding,
)
from datasets import load_dataset

MODEL_NAME   = "allenai/biomed_roberta_base"
TRAIN_CSV    = "filtered_output_document.csv"  # your training file
PREDICT_CSV  = "predict.csv"          # your unlabeled file
OUTPUT_DIR   = "./biomed-roberta-finetuned"
LABEL_LIST   = ["Normal", "Abnormal", "Uncertain"]
NUM_LABELS   = len(LABEL_LIST)
MAX_LENGTH   = 512
BATCH_SIZE   = 8
LR           = 2e-5
EPOCHS       = 10

# ── LOAD TOKENIZER & MODEL ───────────────────────────────────────────────
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model     = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=NUM_LABELS
)

# ── PREPARE TRAINING DATA ─────────────────────────────────────────────────
raw_ds = load_dataset("csv", data_files={"train": TRAIN_CSV})

def tokenize_and_label(example):
    # 1) tokenize the “Content”
    encodings = tokenizer(
        example["Content"],
        truncation=True,
        padding="max_length",
        max_length=MAX_LENGTH
    )
    # 2) convert text label → int
    encodings["labels"] = LABEL_LIST.index(example["Classification"])
    return encodings

train_ds = raw_ds["train"].map(
    tokenize_and_label,
    batched=False,
    remove_columns=raw_ds["train"].column_names
)
train_ds.set_format(
    type="torch",
    columns=["input_ids", "attention_mask", "labels"]
)

# ── SET UP TRAINER ────────────────────────────────────────────────────────
data_collator = DataCollatorWithPadding(tokenizer)
training_args = TrainingArguments(
    output_dir          = OUTPUT_DIR,
    num_train_epochs    = EPOCHS,
    per_device_train_batch_size = BATCH_SIZE,
    learning_rate       = LR,
    logging_dir         = f"{OUTPUT_DIR}/logs",
    logging_steps       = 100,
    save_steps          = 500,
    save_total_limit    = 2,
    load_best_model_at_end = False,
)
trainer = Trainer(
    model          = model,
    args           = training_args,
    train_dataset  = train_ds,
    tokenizer      = tokenizer,
    data_collator = data_collator,
)

# ── FINE-TUNE! ────────────────────────────────────────────────────────────
trainer.train()
trainer.save_model(OUTPUT_DIR)
print(f"[✔] Model fine-tuned & saved to {OUTPUT_DIR}")

import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Load model and tokenizer
MODEL_DIR = "./biomed-roberta-finetuned"
LABEL_LIST = ["Normal", "Abnormal", "Uncertain"]

tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)
model.eval()

# Load data
df = pd.read_csv("predict_data_10.csv")
contents = df["Content"].tolist()

# Set up batch inference
BATCH_SIZE = 8
predicted_labels = []
for i in range(0, len(contents), BATCH_SIZE):
    batch_texts = contents[i:i + BATCH_SIZE]
    batch_encodings = tokenizer(
        batch_texts,
        truncation=True,
        padding=True,
        max_length=512,
        return_tensors="pt"
    )

    with torch.no_grad():
        outputs = model(**batch_encodings)
        batch_preds = torch.argmax(outputs.logits, dim=1).tolist()
        predicted_labels.extend(batch_preds)

# Map predictions to label names
df["Predicted_Label"] = [LABEL_LIST[i] for i in predicted_labels]

# Save
df.to_csv("predicted_output.csv", index=False)
print("[✔] Saved predictions to predicted_output.csv")

